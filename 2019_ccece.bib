% Encoding: UTF-8

@Article{barr1994,
  author  = {Barron, Andrew},
  title   = {{[Neural Networks: A Review from Statistical Perspective]: Comment}},
  journal = {Statistical Science},
  year    = {1994},
  volume  = {9},
  number  = {1},
  pages   = {2-30},
  month   = feb,
}

@Article{cheng1994,
  author  = {Cheng, Bing and Titterington, D. M.},
  title   = {{Neural Networks: A Review from Statistical Perspective}},
  journal = {Statistical Science},
  year    = {1994},
  volume  = {9},
  number  = {1},
  pages   = {2-30},
  month   = feb,
}

@PhdThesis{mann1999,
  author = {Mann, Iain},
  title  = {{An Investigation of Nonlinear Speech Synthesis and Pitch Modification Techniques}},
  school = {The University of Edinburgh},
  year   = {1999},
}

@Article{fried1981,
  author  = {Friedman, J. H. and Stuetzle, W.},
  title   = {{Projection Pursuit Regression}},
  journal = {Journal of American Statistical Association},
  year    = {1981},
}

@InProceedings{lowe1991,
  author    = {Lowe, D.},
  title     = {{On the Statistical Inversion of RBF Networks: a Statistical Interpretation}},
  booktitle = {Second IEE International Conference on Artificial Neural Networks},
  year      = {1991},
}

@Article{zhang1992,
  author  = {Zhang, D. and Benveniste, A.},
  title   = {{Wavelet Networks}},
  journal = {IEEE Trans. Neural Networks},
  year    = {1992},
}

@Article{chen1994,
  author   = {D. S. Chen and R. C. Jain},
  title    = {A robust backpropagation learning algorithm for function approximation},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {1994},
  volume   = {5},
  number   = {3},
  pages    = {467-479},
  month    = {May},
  issn     = {1045-9227},
  doi      = {10.1109/72.286917},
  keywords = {feedforward neural nets;backpropagation;function approximation;iterative methods;convergence of numerical methods;backpropagation;learning algorithm;function approximation;multilayer feedforward neural networks;input-output mappings;objective function;iteration time;nonlinear cascade;convergence;Backpropagation algorithms;Multi-layer neural network;Noise robustness;Neural networks;Feedforward neural networks;Training data;Approximation algorithms;Noise shaping;Parametric statistics;Shape},
}

@Book{haya2000,
  title     = {Econometrics},
  publisher = {Princeton University Press},
  year      = {2000},
  author    = {Fumio Hayashi},
}

@Article{zhang2004,
  author   = {Xi Min Zhang and Yan Qiu Chen and Nirwan Ansari and Yun Q. Shi},
  title    = {Mini-max initialization for function approximation},
  journal  = {Neurocomputing},
  year     = {2004},
  volume   = {57},
  pages    = {389 - 409},
  issn     = {0925-2312},
  note     = {New Aspects in Neurocomputing: 10th European Symposium on Artificial Neural Networks 2002},
  abstract = {Neural networks have been successfully applied to various pattern recognition and function approximation problems. However, the training process remains a time-consuming procedure that often gets stuck in a local minimum. The optimum network size and topology are usually unknown. In this paper, we formulate the concept of extrema equivalence for estimating the complexity of a function. Based on this formulation, the optimal network size and topology can be selected according to the number of extrema. Mini-max initialization method is then proposed to select the initial values of the weights for the network that is proven to greatly speed up training. The superior performance of our method in terms of convergence and generalization has been substantiated by experimental results.},
  doi      = {https://doi.org/10.1016/j.neucom.2003.10.014},
  keywords = {Extrema equivalence, Random initialization, Mini-max initialization, Promising area, Chessboard initialization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231203005228},
}

@Book{du2014,
  title     = {{Neural Networks and Statistical Learning}},
  publisher = {Springer, London},
  year      = {2014},
  author    = {Du, KL. and Swamy, M.N.S.},
}

@Comment{jabref-meta: databaseType:bibtex;}
